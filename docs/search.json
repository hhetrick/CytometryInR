[
  {
    "objectID": "Schedule.html",
    "href": "Schedule.html",
    "title": "Cytometry in R",
    "section": "",
    "text": "Cytometry in R: A Course for Beginners\nLast updated: Wednesday, December 17, 2025\nPlease complete the interest form to stay in the loop as we finalize date and times. If you would like us to cover additional topics, please suggest them to the email listed in the interest form. Thanks! - David\n \n\n\nWorkstation Setup\n\nWeek 1: During this first session, we will ensure that everyone’s computer gets properly setup, as well as start building individual participants familiarity with the software infrastructure that they will be using throughout the rest of the course. Namely, you will need to install R, Rtools, Positron, Quarto, and Git; as well as setup and link to a GitHub account. This session also touches on concepts behind version control and how to install R package from the various repositories (CRAN, Bioconductor, GitHub).\n \n\n\nFile Paths\n\nWeek 2: For this session, we focus on how to programmatically tell your computer where to locate your experimental files, introducing the concept of file paths. We explore how the various operating systems (Linux, MacOS, Windows) specify their respective folders and files, and how to identify where you are currently within the directory. Our goal by the end of this session is to have walked you through how to figure out where an .fcs file of interest is stored, and convey to your computer where you want it copied/moved to, without encountering the common pitfalls.\n \n\n\nInside an .FCS file\n\nWeek 3: In the course of this session we will slice into an .FCS file and find out what the individual components that make it up are. In the process, we will cover the concepts of main data structures within R (vectors, matrices, data.frames, list) and how to identify what we are working with. Additionally, we will explore how various cytometry softwares store their metadata variables under various keywords that can be useful to know about.\n \n\n\nIntroduction to the Tidyverse\n\nWeek 4: Within this session, we explore how the various tidyverse packages can be utilized to reorganize rows and columns of data in ways that are useful for data analysis. We will primarily work with the MFI expression data we isolated from within the .fcs file in the previous session, identifying and isolating events that meet certain criterias. We introduce the concepts behind “tidy” data and how it can improve our workflows.\n \n\n\nGating Sets\n\nWeek 5: As part of this session, we learn about the two main flow cytometry infrastructure packages in R we will be working with during the course, flowcore and flowWorkspace. Throughout the session, we will compare how they differ in naming, memory usage, and accessing .fcs file metadata. We additionally explore how to add keywords to their respective metadata for use in filtering specimens of interest from the larger set of .fcs files.\n \n\n\nVisualizing with ggplot2\n\nWeek 6: During this session we provide an introduction to the ggplot2 package. We will take the datasets we have collected from the previous sessions and see how in varying in different arguments at the respective plot layers we can produce and customize many different forms of plots, focusing on both cytometry and statistics plots. We close out providing links to additional helpful resources and highlight the TidyTuesday project.\n \n\n\nApplying Transformations and Compensation\n\nWeek 7: For this seventh session, we take a closer look at the raw values of the data within our .fcs files, and explore the various ways to transform (ie. scale) flow cytometry data in R to better visualize “positive” and “negative populations”. In the process, we visualize the differences resulting from applying different transformations commonly used by commercial software. Similarly, we learn how to apply and visualize compensation in context of conventional flow cytometry files.\n \n\n\nManual and Automated Gating\n\nWeek 8: Within this session, we explore various ways to implement gating for flow cytometry files in R. We will explore manual approaches utilizing flowGate, as well as automated options with openCyto and it’s gating templates. We additionally will explore how to provide gate constraints and various ways to visually screen and evaluate the outcomes within the context of our own projects.\n \n\n\nIt’s Raining Functions!\n\nWeek 9: In the course of this ninth session, we tackle one of the harder but most useful concepts to learn for a begginer, namely functions. We explore what they are, how their individual arguments work, how they differ from for-loops, and how to create our own to do useful work, reduce the number times code gets copied and pasted. Additionally, some functional programming best practices will be introduced, as well as provide introduction to how to use the walk and map functions from the purrr package.\n \n\n\nDownsampling and Concatenation\n\nWeek 10: Within this session, we will expand on our growing understanding of GatingSets, functions and fcs file internals to write a script to downsample your fcs files to a desired number (or percentage) of cells for a given cell population. We will additionally learn how to concatenate these downsampled files together, and save them to a new .fcs file in ways that the metadata can be read by commercial software without the scaling being widely thrown off.\n \n\n\nRetrieving data for Statistics\n\nWeek 11: Leveraging the increased familiarity working with the various packages this far in the course, in this session we will retrieve summary statistics for the gates within our GatingSet, and programmatically derrive out tidy data.frames for use in statistical analyses typically used by many Immunologist. In the process, we add a couple additional plot types to our ggplot2 arsenal to hold in reserve should Prism prices go up again.\n \n\n\nSpectral Signatures\n\nWeek 12: As part of this session, we will explore how to extract fluorescent signatures from our raw spectral flow cytometry reference controls. Building on prior concepts, we will learn to isolate median signatures from positive and negative gates, and how to derrive and plot normalized signatures. We also introduce plotly package and it’s interactive plotting features, before showcasing various packages attempts at facilitating signature retrieval.\n \n\n\nSimilarities and Hotspots\n\nWeek 13: During this session, we will utilize the spectral signature matrix isolated from raw spectral flow cytometry controls and evaluate different ways of evaluating how similar different fluorescent signatures are to each other. In the process, we will gain better understanding of the metrics behind similarity (cosine), panel complexity (kappa), and unmixing-dependent spreading (collinearity).\n \n\n\nUnmixing in R\n\nWeek 14: In the course of this session, we will attempt a reach goal of many, namely carry out unmixing of raw .fcs files using the spectral signatures we have isolated from our unmixing controls, and write to new .fcs files. After evaluating the necessary internals, we will explore how various current cytometry R packages have implemented their own unmixing functions, and the various limitations that each approach has encountered.\n \n\n\nCleaning Algorithms\n\nWeek 15: In the span of this session, we will directly compare how various Bioconductor data cleanup algorithms (namely PeacoQC, FlowAI, FlowCut, and FlowClean) tackle distinguishing and removing bad quality events. We will see how they perform with previously identified good quality and horrific quality .fcs files. We will whether the implemented algorithmic decisions made sense, and how to customize them within our workflows to achieve our own desired goals.\n \n\n\nClustering Algorithms\n\nWeek 16: As part of this session, we venture away from supervised and semi-supervised analyses to explore unsupervised clustering approaches, namely FlowSOM and Phenograph. We will compare outcomes depending markers included, transformations applied, and panel used to gain a greater familiarity with how they work. We wrap up by investigating ways to visualize marker expression of cells ending up in each cluster, and how to backgate them to our manual gates.\n \n\n\nNormalization: Batch Effect or Real Biology\n\nWeek 17: During this session, we will dive into evaluating the performance of two commonly used normalization algorithms, CytoNorm and CyCombine. We will utilize our ggplot2 and functional programming toolkits to create a customized workflow to visualize the differences for our respective cell populations before and after normalization, to better evaluate how the respective parameter choices can affect the process.\n \n\n\nDimensionality Visualization\n\nWeek 18: For this session, we explore how dimensionality visualization algorithms perform tSNE and UMAP in R using our raw and unmixed samples. In the process, we will explore how markers included, number of cells, and presence of bad quality events can impact the final visualizations. Finally, we will provide an overview of how to link to Python to additionally run PaCMAP and PHATE visualizations for use in R.\n \n\n\nAnnotating Unsupervised Clusters\n\nWeek 19: In the course of this session, we explore ways to scale our efficiency in figuring out what an unsupervised cluster of cells may be, by employing several annotation packages. We explore how these work under the hood in their decision making process, and how to link them to reference data from external repositories for additional evaluation.\n \n\n\nThe Art of GitHub Diving\n\nWeek 20: Within this session, we delve into the art of investigating a new-to-you GitHub repository. We discuss the overall structure of R packages stored as source files within GitHub repositories, and how to leverage this knowledge when troubleshooting errors thrown by underdocumented R packages. We discuss how to modify identified functions, evaluate them, and process to submit helpful bug reports back to the original project to help fix the issue.\n \n\n\nXML Files All The Way Down\n\nWeek 21: Breaking news alert, most of the experiment templates and worksheet layouts we work with as cytometrist are .xml files. In this session, we learn some additional coding tools to allow us to work with these types of files to extract useful data. In this session, we test out our new problem solving abilities to retrieve data from SpectroFlo and Diva .xml files to monitor how our core’s flow cytometers behaved for various users last week.\n \n\n\nUtilizing Bioconductor packages\n\nWeek 22: Many of the R packages for Flow Cytometry we have utilized in this course were packages from the Bioconductor project. We take a look at what makes Bioconductor packages unique compared to packages found on GitHub and CRAN, explore some of their specific infrastructure types for flow cytometry data, and highlight some useful packages for downstream analysis that we haven’t had time to properly explore.\n \n\n\nBuilding your First R package\n\nWeek 23: For most of the course, we have been working with R packages that other individuals built and maintained. In this session, we leverage all your hard work from the rest of the course and corral the unwieldly arsenal of functions you wrote into your first R package for easier use. We will discuss the individual pieces of an R package, the importance of a well-setup namespace file, and how to generate help page manuals to refer future-you back to what your individual function arguments actually do.\n \n\n\nReproducibility and Replicability\n\nWeek 24: Throughout the course, we emphasized the importance of making your workspaces and code reproducible and replicable. But what do we mean by these terms, and are there best practices we could add to our existing workflow to do this more efficiently? We explore a couple community-led efforts within the cytometry space and troubleshoot their implementation into a previously published pipeline.\n \n\n\nValidating Algorthmic Tools\n\nWeek 25: We will be the first to admit, new implementations of algorithms as R packages are awesome! We appreciate the effort that went into them and making them available to the community at large. But what is the best way of evaluating whether they behave as promised, or work for our dataset? During this session, we share tips and tricks to gain better understanding of how a new R package works, and things to watch out for when evaluating complicated algortithms. We wrap with walkthrough of how to generate simulated datasets with known distributions for use in testing.\n \n\n\nEveryone Get’s a Quarto Website\n\nWeek 26: In this session, we will extend the knowledge of .R and .qmd files you have gained from the course and extend them to create your own website using Quarto. We discuss the additional files that are required, how to customize and render the website locally, and finally set up Quarto Pub or GitHub Pages website that we are to access online.\n \n\n\nOpen Source Licenses\n\nWeek 27: For this course, we have relied extensively on open-source software to create our own data analysis pipelines. In the process, you may have some recollection of the various license names. But what impact do all these different names have in the end? We take a brief deep-dive into the ecosystem of free and open-source licenses, and evaluate what their respective license terms mean for us as individual users of the code, as well as potential developers extending existing codebases.\n \n\n\nDatabases and Repositories\n\nWeek 28: During this session, we will learn how to identify and retrieve .fcs files from databases. While many of us are accustomed to working with large datasets of our own making, many of us are increasingly encountering larger-than-memory datasets, as well as files stored in large repositories. In this session, we will explore several database focused R packages, before investigating how to identify and retrieve .fcs files and associated metadata of interest from repositories, namely ImmPort (and maybe FlowRepository if it can be pinged that afternoon).\n \n\n\nAssembling Web Data\n\nWeek 29: In this session, we briefly delve into the concepts of web-scraping and APIs in general. We highlight useful packages, namely httr2 and rvest, and best practices implemented to allow respectful retrieval of useful data without crashing someone’s server like some AI startup bot. We finish by providing a list of additional useful resources for those interested in learning more.\n \n\n\nFuture Directions\n\nWeek 30: In this final of the planned sessions, we revisit our solutions to the challenge problems set out during the beginning of the course. We also discuss potential future topics to visit in the future, and any additional resources that proved helpful throughout the course."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "License",
    "section": "",
    "text": "Last updated: Wednesday, December 17, 2025\nClick here for the course content and tentative schedule.\nTo stay in the loop as we finalize course details (start date, days, times), please complete the interest form.\n\nMotivation\n\n“Cytometry in R” is a free weekly mini-course being offered in-person and virtually by the Flow Cytometry Shared Resource staff at the University of Maryland Greenebaum Comprehensive Cancer Center. Its primary audience is for those with prior flow cytometry knowledge, who have no-to-limited previous experience with the programming language R.\nWhile many cytometry enthusiast express an interest in learning how to carry out flow cytometry analyses in R, they often do not know where to start. Additionally, many of the limited existing resources are focused towards users with intermediate bioinformatic skills, contributing to a greater barrier for entry for those just starting out. Our motivation in offering this mini-course tailored towards beginners is to make the learning journey smoother than the one we ourselves experienced.\n\n\n\nRationale\n\nWhile designing the course, we kept the following concepts in mind:\n\nBeginning coders benefit both by having detailed examples that they can initially work through on their own time, as well as less defined problems that through troubleshooting enable the acquisition of the thought-process and skills needed for coding.\nSome topics will take individuals a longer time to fully grasp. Providing a format and resources that enable being able to revisit the material multiple times is incredibly helpful. Likewise, life is busy, and missing a workshop session is highly probable. If this happens, it shouldn’t make or break the ability of the individual to understanding the rest of the course.\nConsistency is key, and being able to apply what you are learning to your own datasets, files, and questions of interest helps achieve this.\n\n\n\n\nCost\nIs there a cost to participate? No, it’s absolutely free! Is there a catch? Yes, you learn R, and may wind up with strong feelings about flowframes vs. cytoframes. This is also our first year offering this course, so we will sporadically ask you to fill out a feedback form to help us improve.\n\n\n\n\nFormat\nEach week, the mini-course will cover a particular topic for an hour. This individual class is offered on multiple days, at different times, both in-person and online. You are invited to attend whichever day best fits your schedule for that week. If life gets busy and you can’t make your regular day, you have an additional four opportunities for that week covering the same topic.\nIn-person, we are tentatively planning on Monday, Wednesday, Thursdays from 4-5 pm EST in Bressler 7-035. Virtual options are tentatively planned for Tuesday and Friday via livestream on YouTube and the Cytometry Discord. However, final date and times will be determined on the interest, availability, and timezones of interested particants (please do fill our the interest form).\nEach session starts with 10-15 minutes of Background about the session topic, and it’s relevance to both R and Cytometry. Over the next 30 minutes, participants work through Hands-on examples, using either their own data, or an example dataset that we will provide. Instructors assist where needed and answer questions during this time. The final 10 minutes we reconvene as a group, Share insights and troubleshoot any remaining stickpoints. Finally, we provide links to useful resources for those who want to learn more about the topic, as well as provide two optional take-home problems.\n\n\nEach week, we will update the website with the course materials for the week. These will typically consist of the Quarto Markdown document, which is used to explain and run the R code. If you have your own data, you can bring your own data! If you don’t have some, or want to follow along, we will also make available some of our data that you can utilize. All course materials for the given week will be made available online via our course website and course GitHub repository. In our commitment to open-source and open-science, all teaching materials are freely offered under a CC-BY-SA license, while all code examples are offered under the AGPL3-0 copyleft license.\n\nThe take-home problems are intended to get you to work with your own data on similar problems in a not-so-structured manner. The challenges you encounter in solving them will help foster the problem-solving/debugging/way-of-thinking skills needed to successfully work with code. Both works-and-progress and solved problems can be discussed and submitted on our GitHub repository to the designated take-home folder of the week, where both instructors and others taking the course can provide feedback.\n\n\n\nComputing Requirements\n\nFor those attending online, you will need a computer with internet access. Operating system shouldn’t matter, as we will be offering code examples for Linux, Mac and Windows. As with all things flow-cytometry software, a good CPU with multiple cores, more RAM, and greater storage space are generally helpful, but not deal breakers.\n\nYou will need to be able to install the required software (R, Rtools, Positron, Quarto, and Git) as well as intstall and compile R packages from the CRAN and Bioconductor repositories (as well as a few GitHub-based R packages). For those using university or company administered computers, please be aware that you may not have the necessary permissions to install these directly, and may need to reach out to your IT department to help get these initial requirements set up. If you are using your own computer, congratulations, you are your system administrator, and should already have the necessary permissions.\n\n\nFor those attending in-person, we have set up a pop-up computer lab in the conference room. For those who arrive early, we have a limited number of second screens with provided mouse and keyboard that you can plug a laptop into via HDMI cable to set up a workstation. For those arriving later, the room has enough space (and electrical plugs) for 20 people, but you will need to balance a laptop on your lap. If you have your own laptop, feel free to bring it. If you don’t have a laptop, the flow core has 6 loaner laptops running Linux that we can let participants use for that session.\n\n\nSchedule\nClick here to see the preliminary course content and schedule as of December 17, 2025. Please complete the interest form to stay in the loop as we finalize date and times. Thanks!\n\n\nLicense\nThe course material is licensed under under the Creative Commons Attribution-Share Alike 4.0 International License. The code examples provided in this course are licensed under the GNU AFFERO GENERAL PUBLIC LICENSE (AGPL-3.0)"
  }
]